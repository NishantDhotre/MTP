{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 17:55:50.710246: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/surajit/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2024-01-18 17:55:50.710269: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_regression as MIR\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE seg-areas  \n",
    "SEGMENT_CLASSES = {\n",
    "    0 : 'NOT tumor',\n",
    "    1 : 'NECROTIC/CORE', \n",
    "    2 : 'EDEMA',\n",
    "    3 : 'ENHANCING' # original 4 -> converted into 3 later\n",
    "}\n",
    "\n",
    "# # days start interval\n",
    "# SURVIVAL_CATEGORIES= {\n",
    "#     'SHORT' : 0 , # 0-300\n",
    "#     'MEDIUM' : 250,  # 300-450\n",
    "#     'LONG' : 450, # 450 and more\n",
    "# }\n",
    "\n",
    "# there are 155 slices per volume\n",
    "# to start at 5 and use 145 slices means we will skip the first 5 and last 5 \n",
    "VOLUME_SLICES = 100 \n",
    "VOLUME_START_AT = 22 # first slice of volume that we will include\n",
    "IMG_SIZE=128\n",
    "TRAIN_DATASET_PATH='./MICCAI_BraTS2020_TrainingData/'\n",
    "VAL_DATASET_PATH='./MICCAI_BraTS2020_ValidationData/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of directories with studies\n",
    "train_and_val_directories = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]\n",
    "train_and_val_directories+= [f.path for f in os.scandir(VAL_DATASET_PATH) if f.is_dir()]\n",
    "# file BraTS20_Training_355 has ill formatted name for for seg.nii file\n",
    "train_and_val_directories.remove(TRAIN_DATASET_PATH+'BraTS20_Training_355')\n",
    "\n",
    "\n",
    "def pathListIntoIds(dirList):\n",
    "    x = []\n",
    "    for i in range(0,len(dirList)):\n",
    "        x.append(dirList[i][dirList[i].rfind('/')+1:])\n",
    "    return x\n",
    "\n",
    "train_and_test_ids = pathListIntoIds(train_and_val_directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 119 lines.\n",
      "34 38 46\n",
      "1767\n",
      "Processed 119 lines.\n",
      "34 38 46\n",
      "1767\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "csvs = [r'MICCAI_BraTS2020_TrainingData/survival_info.csv', r'MICCAI_BraTS2020_ValidationData/survival_evaluation.csv']\n",
    "\n",
    "age_dict = {}\n",
    "days_dict = {}\n",
    "at_line = 0\n",
    "category_short = 0\n",
    "category_medium = 0\n",
    "category_long = 0\n",
    "max_days = 0\n",
    "for csv_path in csvs:\n",
    "    with open(csv_path, mode='r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file,delimiter = ',')\n",
    "    #  row_count = sum(1 for row in csv_reader)\n",
    "    #   print(f'total rows: {row_count} .')\n",
    "        for row in csv_reader:\n",
    "            if at_line == 0:\n",
    "                # print(f'Column names are {\", \".join(row)}')\n",
    "                at_line += 1\n",
    "            else:\n",
    "                if (row[-1] != \"GTR\"):\n",
    "                    continue\n",
    "                # print(row)\n",
    "                key = row[0]\n",
    "                age = row[1]\n",
    "                days = row[2]\n",
    "                age_dict[key] = float(age)\n",
    "                if (not days.isnumeric()):\n",
    "                    continue\n",
    "                days_dict[key] = int(days)\n",
    "                max_days = max(max_days,int(days))\n",
    "                if int(days) < 250:\n",
    "                    category_short += 1\n",
    "                elif (int(days) >= 250 and int(days) <= 450):\n",
    "                    category_medium += 1\n",
    "                else:\n",
    "                    category_long += 1\n",
    "                at_line+=1\n",
    "\n",
    "    print(f'Processed {at_line} lines.')\n",
    "    print(category_short,category_medium,category_long)\n",
    "    print(max_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def col_and_eval(ch_ft = []):\n",
    "    id_list2 = []\n",
    "    csv_filenames = [r'./results.csv',r'./results_val.csv']\n",
    "    dt =[]\n",
    "    jj = [] #key_list\n",
    "    id_tp = 0\n",
    "    row_cc = 0\n",
    "    row2 = {}\n",
    "    id_check = {}\n",
    "    for csv_filename in csv_filenames:\n",
    "        with open(csv_filename) as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                # if row_cc > 20:\n",
    "                #     break\n",
    "                # row_cc += 1\n",
    "                dt_tp = row\n",
    "                if row['Patient_ID'] not in id_check:\n",
    "                    id_check[row['Patient_ID']] = 0\n",
    "                id_check[row['Patient_ID']] += 1\n",
    "                if len(id_list2) != 0:\n",
    "                    if id_list2[-1] == row[\"Patient_ID\"]:\n",
    "                        id_tp += 1\n",
    "                    else:\n",
    "                        dt.append(row2)\n",
    "                        row2 = {}\n",
    "                        id_tp = 0\n",
    "                if id_tp == 0:\n",
    "                    id_list2.append(row[\"Patient_ID\"])\n",
    "                if len(id_list2) == 1:\n",
    "                    for keysj in row:\n",
    "                        if keysj != None:\n",
    "                            if 'diagnostic' in keysj or 'Modality' in keysj or 'Patient_ID' in keysj:\n",
    "                                continue\n",
    "                            # print(type(keysj),keysj, type(row[keysj]),row[keysj])\n",
    "                            row[keysj] = eval(row[keysj])\n",
    "                            if isinstance(row[keysj],tuple):\n",
    "                                ct = 0\n",
    "                                for it in row[keysj]:\n",
    "                                    keysjn = keysj + str(ct)\n",
    "                                    keysjn += row[\"Modality\"][row[\"Modality\"].find('_',19,-1):row[\"Modality\"].find('.')]\n",
    "                                    if (keysjn in ch_ft) or (len(ch_ft) == 0):\n",
    "                                        jj.append(keysjn)\n",
    "                                    ct += 1\n",
    "                                continue\n",
    "                            keysj += row[\"Modality\"][row[\"Modality\"].find('_',19,-1):row[\"Modality\"].find('.')]\n",
    "                            if (keysj in ch_ft) or (len(ch_ft) == 0):\n",
    "                                jj.append(keysj)\n",
    "                for keysj in row:\n",
    "                    if keysj != None:\n",
    "                        if 'diagnostic' in keysj or 'Modality' in keysj or 'Patient_ID' in keysj:\n",
    "                            continue\n",
    "                        # print(type(keysj),keysj, type(row[keysj]),row[keysj])\n",
    "                        row[keysj] = eval(str(row[keysj]))\n",
    "                        keys2j = keysj + row[\"Modality\"][row[\"Modality\"].find('_',19,-1):row[\"Modality\"].find('.')]\n",
    "                        if isinstance(row[keysj],tuple):\n",
    "                            ct = 0\n",
    "                            for it in row[keysj]:\n",
    "                                keys2j =  keysj + str(ct) + row[\"Modality\"][row[\"Modality\"].find('_',19,-1):row[\"Modality\"].find('.')]\n",
    "                                row2[keys2j] = it\n",
    "                                ct += 1\n",
    "                            continue\n",
    "                        row2[keys2j] = row[keysj]\n",
    "                row2['Patient_ID'] = row['Patient_ID']\n",
    "    dt.append(row2)    \n",
    "    return dt, jj\n",
    "\n",
    "# print(jj)\n",
    "# print(dt)\n",
    "# create only age: category data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patlist_tr = []\n",
    "patlist_val = []\n",
    "\n",
    "# id: age, categories\n",
    "def getListAgeDays(id_list, dt, jj, ch_ft = []):\n",
    "    x_val = []\n",
    "    y_val = []\n",
    "    x_test = []\n",
    "    for i in dt:\n",
    "        if ((i[\"Patient_ID\"] not in days_dict) and (\"Validation\" not in i[\"Patient_ID\"])):\n",
    "            # print(i[\"Patient_ID\"])\n",
    "            continue\n",
    "        if (i[\"Patient_ID\"] not in age_dict):\n",
    "            continue\n",
    "        j = [age_dict[i[\"Patient_ID\"]]]\n",
    "        for keysj in i:\n",
    "            if keysj != None:\n",
    "                if 'diagnostic' in keysj or 'Modality' in keysj or 'Patient_ID' in keysj:\n",
    "                    continue\n",
    "                if (keysj not in ch_ft) and (len(ch_ft) != 0):\n",
    "                    continue\n",
    "                # if(i[keysj][0]=='('):\n",
    "                #     i[keysj] = tuple(i[keysj])\n",
    "                # else:\n",
    "                # print(i[keysj])\n",
    "                # i[keysj] = eval(i[keysj])\n",
    "                # if isinstance(i[keysj],tuple):\n",
    "                #     ct = 0\n",
    "                #     for it in i[keysj]:\n",
    "                #         keysjn = keysj + str(ct)\n",
    "                #         j.append(it)\n",
    "                #         ct += 1\n",
    "                #     continue\n",
    "                j.append(i[keysj])\n",
    "        # print(j)\n",
    "        if \"Validation\" in i[\"Patient_ID\"]:\n",
    "            # print(\"vvv\"+i[\"Patient_ID\"])\n",
    "            x_test.append(j)\n",
    "            patlist_val.append(i[\"Patient_ID\"])\n",
    "        else:\n",
    "            x_val.append(j) \n",
    "            y_val.append(days_dict[i[\"Patient_ID\"]])\n",
    "            patlist_tr.append(i[\"Patient_ID\"])\n",
    "        # if (days_dict[i[\"Patient_ID\"]] < 250):\n",
    "        #     y_val.append(0)\n",
    "        # elif (days_dict[i[\"Patient_ID\"]] >= 250 and days_dict[i[\"Patient_ID\"]] < 450):\n",
    "        #     y_val.append(1)\n",
    "        # else:\n",
    "        #     y_val.append(2)\n",
    "            \n",
    "    return np.array(x_val), np.array(y_val), np.array(x_test)\n",
    "\n",
    "dt, jj = col_and_eval()\n",
    "X_all, y_all, x_val = getListAgeDays(train_and_test_ids,dt,jj)\n",
    "print(len(train_and_test_ids))\n",
    "print(X_all.shape)\n",
    "print(y_all.shape)\n",
    "print(x_val.shape)\n",
    "print(f'X_all: {X_all.shape}')\n",
    "columns = [\"age\"]+jj+[\"prediction\"]\n",
    "print(columns)\n",
    "dfn = np.concatenate((X_all, y_all[:,None]), axis=1)\n",
    "print(dfn.shape,len(columns))\n",
    "df = pd.DataFrame(dfn, columns = columns)\n",
    "df2 = pd.DataFrame(x_val,columns = [\"age\"]+jj)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "v = X_all\n",
    "v_scaled = scaler.fit_transform(v)\n",
    "X_all = v_scaled\n",
    "X_val = scaler.fit_transform(x_val)\n",
    "\n",
    "df = pd.DataFrame(X_all, columns = [\"age\"]+jj)\n",
    "# display(df)\n",
    "df2 = pd.DataFrame(X_val,columns = [\"age\"]+jj)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X_all,y_all,test_size = 0.2, random_state = 42, shuffle = True)\n",
    "X_train = X_all\n",
    "y_train = y_all\n",
    "print(\"x_train shape:\",X_train.shape)\n",
    "# print(\"x_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\",y_train.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfeature.function.similarity_based import fisher_score\n",
    "\n",
    "print(X_train.shape,y_train.shape)\n",
    "_ranks = fisher_score.fisher_score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"age\"]+jj\n",
    "colnew = []\n",
    "def prune2(X):\n",
    "    xx = []\n",
    "    for row in X:\n",
    "        iter = 0\n",
    "        xxtp = []\n",
    "        for col in row:\n",
    "            if _ranks[iter]<400:\n",
    "                xxtp.append(col)\n",
    "                if columns[iter] not in colnew:\n",
    "                    colnew.append(columns[iter])\n",
    "            iter += 1\n",
    "        xx.append(xxtp)\n",
    "    return xx\n",
    "\n",
    "x_train = np.array(prune2(X_train))\n",
    "x_test = np.array(prune2(X_val))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "df = pd.DataFrame(x_train, columns = colnew)\n",
    "df2 = pd.DataFrame(x_test,columns = colnew)\n",
    "# df2.head()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "v = x_train\n",
    "v_scaled = scaler.fit_transform(v)\n",
    "x_train = v_scaled\n",
    "v = x_test\n",
    "v_scaled = scaler.fit_transform(v)\n",
    "x_test = v_scaled\n",
    "\n",
    "df = pd.DataFrame(x_train, columns = colnew)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X_all,y_all,test_size = 0.2, random_state = 42, shuffle = True)\n",
    "# x_train = ch_X_all\n",
    "# y_train = ch_y_all\n",
    "# x_test = ch_x_test\n",
    "print(\"x_train shape:\",x_train.shape)\n",
    "# print(\"x_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\",y_train.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**# mrmr implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(X,y,nbSolutions,I_xy):\n",
    "    f_mir = []\n",
    "    for i in range(int(X.shape[1])):\n",
    "        f_mir.append([I_xy[i],i])\n",
    "    f_mir.sort(reverse=True)\n",
    "    S=[]\n",
    "    si = []\n",
    "    for i in range(nbSolutions):\n",
    "        si.append(f_mir[i][1])\n",
    "        S.append(si)\n",
    "        si = []\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(f,X,y,Si,I_xy,I_xx):\n",
    "    if f == -1:\n",
    "        return float(\"-INF\")\n",
    "    mi_temp = 0\n",
    "    qj = I_xy[f]\n",
    "    for xi in Si:\n",
    "        mi_temp += I_xx[xi][f]\n",
    "    mi_temp/=len(Si)\n",
    "    qj += mi_temp\n",
    "    return qj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mutual Implementation\n",
    "\n",
    "I_xx = np.zeros((x_train.shape[1],x_train.shape[1]))\n",
    "I_xy = np.zeros(x_train.shape[1])\n",
    "print(I_xx.shape,I_xy.shape)\n",
    "\n",
    "i1=0\n",
    "for f1 in x_train.T:\n",
    "    # print(res)\n",
    "    # I_xx[i1][i2] = (-0.5)*math.log(1-(res*res))\n",
    "    I_xx[i1] = MIR(x_train,list(f1))\n",
    "    i1 += 1\n",
    "    print(f\"Processed MI with {i1} features so far...\")\n",
    "\n",
    "#each iteration taking ~30s and 10702 such iterations. thus, IMPOSSIBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_xy = MIR(x_train,list(y_train))\n",
    "\n",
    "S= rank(x_train,y_train,10,I_xy)\n",
    "\n",
    "lenSol = 25\n",
    "nbSol = 10\n",
    "l = 1\n",
    "\n",
    "while l<lenSol:\n",
    "    i = 0\n",
    "    # print(S)\n",
    "    while i< nbSol:\n",
    "        c = -1\n",
    "        for f in range(int(x_train.shape[1])):\n",
    "            if f in S[i]:\n",
    "                continue\n",
    "            if score(f,x_train,y_train,S[i],I_xy,I_xx)>=score(c,x_train,y_train,S[i],I_xy,I_xx):\n",
    "                flag = 1\n",
    "                for j in range(nbSol):\n",
    "                    if j==i:\n",
    "                        continue\n",
    "                    if f not in S[j]:\n",
    "                        flag = 0\n",
    "                    for k in S[i]:\n",
    "                        if k not in S[j]:\n",
    "                            flag = 0\n",
    "                if flag:\n",
    "                    continue\n",
    "                c = f\n",
    "        S[i].append(c)\n",
    "        i += 1\n",
    "    l += 1\n",
    "print(len(S),len(S[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(X,Si):\n",
    "    xx = []\n",
    "    for row in X:\n",
    "        iter = 0\n",
    "        xxtp = []\n",
    "        for col in row:\n",
    "            if iter in Si:\n",
    "                xxtp.append(col)\n",
    "            iter += 1\n",
    "        xx.append(xxtp)\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import statistics\n",
    "from scipy import stats\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    print(predictions)\n",
    "    med_arr = [(yt-yp)*(yt-yp) for yt,yp in zip(predictions,test_labels)]\n",
    "    return mean_squared_error(test_labels,predictions),statistics.median(med_arr),statistics.stdev(med_arr),stats.spearmanr(test_labels,predictions),predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classif(y_train,y_pred):  \n",
    "    y_train_v = []\n",
    "    y_pred_v = []\n",
    "    for yi in y_train:\n",
    "        if (yi < 300):\n",
    "            y_train_v.append(0)\n",
    "        elif (yi >= 300 and yi < 450):\n",
    "            y_train_v.append(1)\n",
    "        else:\n",
    "            y_train_v.append(2)\n",
    "\n",
    "    for yi in y_pred:\n",
    "        if (yi < 300):\n",
    "            y_pred_v.append(0)\n",
    "        elif (yi >= 300 and yi < 450):\n",
    "            y_pred_v.append(1)\n",
    "        else:\n",
    "            y_pred_v.append(2)\n",
    "    return y_train_v,y_pred_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(patlist_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mac = 0\n",
    "fpred = []\n",
    "\n",
    "for Si in S:\n",
    "    X_train_ = np.array(prune(x_train,Si))\n",
    "    X_test_ = np.array(prune(x_test,Si))\n",
    "\n",
    "    print(X_train_.shape)\n",
    "    print(X_test_.shape)\n",
    "    rf = RandomForestRegressor()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=3, random_state=42, n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "    print(X_train_.shape)\n",
    "    rf_random.fit(X_train_, y_train)\n",
    "    rf_random.best_params_\n",
    "    best_grid = rf_random.best_estimator_\n",
    "    meanSE, medianSE, stdSE, spearmanR, y_pred = evaluate(best_grid, X_train_, y_train)\n",
    "    print(meanSE, medianSE, stdSE, spearmanR)\n",
    "    y_pred2=best_grid.predict(X_test_)\n",
    "    print(y_pred2)\n",
    "    for hh in y_pred2:\n",
    "        print(hh)\n",
    "    y_train_v, y_pred_v = classif(y_train,y_pred)\n",
    "    cm = confusion_matrix(y_train_v, y_pred_v)\n",
    "    acc_sc = accuracy_score(y_train_v, y_pred_v)\n",
    "    print(acc_sc)\n",
    "    if acc_sc>mac:\n",
    "        print(f\"model updated as training accuracy ={acc_sc} is better than last model which had accracy = {mac}\")\n",
    "        mac = acc_sc\n",
    "        fpred = y_pred2\n",
    "\n",
    "    sns.set(font_scale=1.2) \n",
    "    sns.heatmap(cm, annot=True, annot_kws={\"size\": 12}) \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xlsxwriter module\n",
    "import xlsxwriter\n",
    "\n",
    "workbook = xlsxwriter.Workbook('.\\mrmre_e_op.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "# Start from the first cell.\n",
    "# Rows and columns are zero indexed.\n",
    "row = 0\n",
    "column = 0\n",
    "\n",
    "# iterating through content list\n",
    "for item in patlist_val :\n",
    "\n",
    "\t# write operation perform\n",
    "\tworksheet.write(row, column, item)\n",
    "\n",
    "\t# incrementing the value of row by one\n",
    "\t# with each iterations.\n",
    "\trow += 1\n",
    "\t\n",
    "column += 1\n",
    "row = 0\n",
    "for item in fpred :\n",
    "\n",
    "\t# write operation perform\n",
    "\tworksheet.write(row, column, item)\n",
    "\n",
    "\t# incrementing the value of row by one\n",
    "\t# with each iterations.\n",
    "\trow += 1\n",
    "\n",
    "workbook.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = [patlist_val,fpred]\n",
    "ab = zip(*ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('.\\mrmre_e_op.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    row_list = list(ab)\n",
    "    \n",
    "    writer.writerows(row_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
