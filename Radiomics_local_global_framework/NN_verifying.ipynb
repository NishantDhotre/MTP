{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "import csv\n",
    "from sklearn.linear_model import LassoCV\n",
    "from stg import STG\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_used = 'Neural_Network'\n",
    "print_interval = 5000\n",
    "epochs = 10000\n",
    "selection_method = 'Stochastic Gates'\n",
    "# selection_method = 'lasso'\n",
    "\n",
    "Light_GBM_global = [\n",
    "    # \"flair_t1ce_t2\",\n",
    "    # \"flair_t1ce_t2\",\n",
    "    # \"flair\",\n",
    "    \"flair_t1ce\",\n",
    "    # \"flair_t1ce\"\n",
    "]\n",
    "Light_GBM_local = [\n",
    "    # \"flair_t1ce_t2\",\n",
    "    \"flair_t1ce\",\n",
    "    # \"flair\",\n",
    "    # \"t2\",\n",
    "    # \"flair_t1ce\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_directory_exists(filepath):\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_radiomic_fetures(target_directory, file_name):\n",
    "    # Load the numpy array from the file in the target directory\n",
    "    file_path = os.path.join(target_directory, file_name)\n",
    "    array = np.load(file_path)\n",
    "    print(f\"Array loaded from '{file_path}'\")\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_parameters(modality_used,  model_class):\n",
    "    model_dir = f\"./models/{selection_method}_feature_selection/{model_used}/{modality_used}/\"\n",
    "    \n",
    "    model_file = os.path.join(model_dir, 'model.pt')\n",
    "    params_file = os.path.join(model_dir, 'params.txt')\n",
    "    mask_file = os.path.join(model_dir, f'{selection_method}_mask.npy')\n",
    "    # Load the selected features mask\n",
    "    mask = np.load(mask_file)\n",
    "    inputdim = np.count_nonzero(mask)\n",
    "    # Load the model\n",
    "    model = model_class(inputdim)\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Load the parameters\n",
    "    with open(params_file, 'r') as file:\n",
    "        params = file.read().strip()\n",
    "    \n",
    "    \n",
    "    print(f\"Model, parameters, and {selection_method} mask loaded successfully for modality {modality_used}.\")\n",
    "    \n",
    "    return model, params, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_features(modality_keys, dataset_type):\n",
    "    combined_features = []\n",
    "    for modality in modality_keys:\n",
    "        # Load the features for each modality\n",
    "        features = np.load(f'../local_spatial_Framework/features/{modality}/{dataset_type}/{dataset_type}_backbone_outputs.npy')\n",
    "        combined_features.append(features)\n",
    "    # Combine features along the feature dimension (axis=1)\n",
    "    return np.concatenate(combined_features, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_csv(y_pred_validation, modality_used):\n",
    "    df = pd.read_csv('../dataset/MICCAI_BraTS2020_ValidationData/survival_evaluation.csv')\n",
    "    validation_ids = df['BraTS20ID'].values\n",
    "    \n",
    "    #removing 116 paitents ID from list\n",
    "    validation_ids = np.delete(validation_ids, 26, axis=0)\n",
    "    \n",
    "    filename = f\"../radiomics_local_global_predictions/{selection_method}_feture_selection/{model_used}/{modality_used}_{model_used}_verifying_again.csv\"\n",
    "\n",
    "    ensure_directory_exists(filename)\n",
    "\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"ID\", \"Days\"])\n",
    "        for id, day in zip(validation_ids, y_pred_validation):\n",
    "            writer.writerow([id, day])\n",
    "\n",
    "    print(f\"CSV file '{filename}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(modality_used):\n",
    "    base_dir = os.path.join('../Global_extracted_features', modality_used)\n",
    "    train_features = np.load(os.path.join(base_dir, 'train_features.npy'))\n",
    "    validate_features = np.load(os.path.join(base_dir, 'validate_features.npy'))\n",
    "    train_labels = np.load(os.path.join(base_dir, 'train_labels.npy'))\n",
    "    return train_features, validate_features, train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd model\n",
    "# class SimpleNN(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(SimpleNN, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 128)\n",
    "#         self.fc2 = nn.Linear(128, 64)\n",
    "#         self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 200)\n",
    "        self.fc2 = nn.Linear(200, 50)\n",
    "        self.fc3 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_prediction(train_features, test_features, train_labels, modality_used):\n",
    "    scaler = StandardScaler()\n",
    "    train_features_scaled = scaler.fit_transform(train_features)\n",
    "    test_features_scaled = scaler.transform(test_features)\n",
    "    print(f\"Size of features before Lasso: {train_features_scaled.shape}\")\n",
    "    \n",
    "    model, param, mask = load_model_and_parameters(modality_used, SimpleNN)\n",
    "    train_features_selected = train_features_scaled[:, mask]\n",
    "    test_features_selected = test_features_scaled[:, mask]\n",
    "\n",
    "    print(f\"Size of features after Stochastic Gates: {train_features_selected.shape}\")\n",
    "      \n",
    "    # Split the selected features into training and validation sets\n",
    "    train_features_final, val_features_final, train_labels_final, val_labels_final = train_test_split(\n",
    "        train_features_selected, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convert features and labels to PyTorch tensors\n",
    "    train_features_tensor = torch.tensor(train_features_final, dtype=torch.float32).to(device)\n",
    "    train_labels_tensor = torch.tensor(train_labels_final, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    val_features_tensor = torch.tensor(val_features_final, dtype=torch.float32).to(device)\n",
    "    val_labels_tensor = torch.tensor(val_labels_final, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    test_features_tensor = torch.tensor(test_features_selected, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_dataset = TensorDataset(train_features_tensor, train_labels_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_test = model(test_features_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    make_csv(y_pred_test, modality_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array loaded from '../radiomics features/all/radiomics_train.npy'\n",
      "Array loaded from '../radiomics features/all/radiomics_validate.npy'\n",
      "radiomic_Train_fetures (235, 400)\n",
      "radiomic_Validation_fetures (28, 400)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "radiomic_Train_fetures = load_radiomic_fetures(\"../radiomics features/all\", \"radiomics_train.npy\")\n",
    "radiomic_Validation_fetures = load_radiomic_fetures(\"../radiomics features/all\", \"radiomics_validate.npy\")\n",
    "\n",
    "print(\"radiomic_Train_fetures\",radiomic_Train_fetures.shape)\n",
    "print(\"radiomic_Validation_fetures\",radiomic_Validation_fetures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and combining features... \n",
      " local-flair_t1ce\n",
      " global-flair_t1ce\n",
      "global_train_features (236, 744) local_train_features (236, 128)\n",
      "global_validate_features (29, 744) local_validation_features (29, 128)\n",
      "combining all\n",
      "radiomics_local_global_training (235, 1272)\n",
      "radiomics_local_global_Validation (28, 1272)\n",
      "Size of features before Lasso: (235, 1272)\n",
      "Model, parameters, and Stochastic Gates mask loaded successfully for modality global_flair_t1ce___local_flair_t1ce.\n",
      "Size of features after Stochastic Gates: (235, 532)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file '../radiomics_local_global_predictions/Stochastic Gates_feture_selection/Neural_Network/global_flair_t1ce___local_flair_t1ce_Neural_Network_verifying_again.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "for modality_used_global, modality_used_local in zip(Light_GBM_global, Light_GBM_local):\n",
    "    modality_key_local = modality_used_local.split(\"_\")\n",
    "    modality_keys_list_global_features = modality_used_global.split(\"_\")\n",
    "    print(f\"\\nLoading and combining features... \\n local-{modality_used_local}\\n global-{modality_used_global}\")\n",
    "\n",
    "    local_train_features = load_and_combine_features(modality_key_local, 'train')\n",
    "    local_validation_features = load_and_combine_features(modality_key_local, 'validation')\n",
    "    \n",
    "    global_train_features, global_validate_features, train_labels = load_features(modality_used_global)\n",
    "    \n",
    "    print(\"global_train_features\",global_train_features.shape, \"local_train_features\", local_train_features.shape)\n",
    "    print(\"global_validate_features\", global_validate_features.shape, \"local_validation_features\",local_validation_features.shape)\n",
    "\n",
    "\n",
    "    local_global_training_features = np.concatenate((global_train_features, local_train_features), axis=1)\n",
    "    local_global_validation_features = np.concatenate((global_validate_features, local_validation_features), axis=1)\n",
    "    \n",
    "#   To remove the data of absent paitents from the local and global features\n",
    "    train_labels = np.delete(train_labels, 98, axis=0)\n",
    "    local_global_training_features = np.delete(local_global_training_features, 98, axis=0)\n",
    "    local_global_validation_features = np.delete(local_global_validation_features, 26, axis=0)\n",
    "\n",
    "\n",
    "    training_all_features = np.concatenate((local_global_training_features, radiomic_Train_fetures), axis=1)\n",
    "    validation_all_features = np.concatenate((local_global_validation_features,radiomic_Validation_fetures), axis=1)\n",
    "\n",
    "    print(\"combining all\")\n",
    "    print(\"radiomics_local_global_training\",training_all_features.shape)\n",
    "    print(\"radiomics_local_global_Validation\",validation_all_features.shape)\n",
    "\n",
    "    modality_used = 'global_' + modality_used_global + '___local_' + modality_used_local\n",
    "    do_prediction(training_all_features, validation_all_features, train_labels, modality_used)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
