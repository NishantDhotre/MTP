{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "from pycox.models import CoxPH\n",
    "from pycox.evaluation import EvalSurv\n",
    "import torch\n",
    "from torchtuples.practical import MLPVanilla\n",
    "from sklearn.linear_model import LassoCV\n",
    "from torchtuples import optim as ttoptim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_used = 'CoxPH'\n",
    "selection_method = 'lasso'\n",
    "\n",
    "Light_GBM_global = [\n",
    "    \"flair_t1ce_t2\",\n",
    "    \"flair_t1ce_t2\",\n",
    "    \"flair\",\n",
    "    \"flair_t1ce\",\n",
    "    \"flair_t1ce\"\n",
    "]\n",
    "Light_GBM_local = [\n",
    "    \"flair_t1ce_t2\",\n",
    "    \"flair_t1ce\",\n",
    "    \"flair\",\n",
    "    \"t2\",\n",
    "    \"flair_t1ce\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../dataset/MICCAI_BraTS2020_TrainingData/survival_info.csv'\n",
    "val_path = '../dataset/MICCAI_BraTS2020_ValidationData/survival_evaluation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    return df['Events'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensure_directory_exists(filepath):\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_radiomic_features(target_directory, file_name):\n",
    "    file_path = os.path.join(target_directory, file_name)\n",
    "    array = np.load(file_path)\n",
    "    print(f\"Array loaded from '{file_path}'\")\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_parameters(model, modality_used, mask):\n",
    "    model_dir = f\"./models/{selection_method}_feature_selection/{model_used}/{modality_used}/\"\n",
    "    ensure_directory_exists(model_dir)\n",
    "    model_file = os.path.join(model_dir, 'model.pth')\n",
    "    mask_file = os.path.join(model_dir, f'{selection_method}_mask.npy')\n",
    "    \n",
    "    torch.save(model.state_dict(), model_file)\n",
    "    np.save(mask_file, mask)\n",
    "    \n",
    "    print(f\"Model and {selection_method} mask saved successfully for modality {modality_used}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_features(modality_keys, dataset_type):\n",
    "    combined_features = []\n",
    "    for modality in modality_keys:\n",
    "        features = np.load(f'../local_spatial_Framework/features/{modality}/{dataset_type}/{dataset_type}_backbone_outputs.npy')\n",
    "        combined_features.append(features)\n",
    "    return np.concatenate(combined_features, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_csv(y_pred_validation, modality_used):\n",
    "    df = pd.read_csv('../dataset/MICCAI_BraTS2020_ValidationData/survival_evaluation.csv')\n",
    "    validation_ids = df['BraTS20ID'].values\n",
    "    filename = f\"../radiomics_local_global_predictions/{selection_method}_feature_selection/{model_used}/{modality_used}_{model_used}.csv\"\n",
    "\n",
    "    ensure_directory_exists(filename)\n",
    "\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"ID\", \"Days\"])\n",
    "        for id, day in zip(validation_ids, y_pred_validation):\n",
    "            writer.writerow([id, day])\n",
    "\n",
    "    print(f\"CSV file '{filename}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_features(modality_used):\n",
    "    base_dir = os.path.join('../Global_extracted_features', modality_used)\n",
    "    train_features = np.load(os.path.join(base_dir, 'train_features.npy'))\n",
    "    validate_features = np.load(os.path.join(base_dir, 'validate_features.npy'))\n",
    "    train_labels = np.load(os.path.join(base_dir, 'train_labels.npy'))\n",
    "    return train_features, validate_features, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_features, validate_features, train_labels, modality_used):\n",
    "    scaler = StandardScaler()\n",
    "    train_features_scaled = scaler.fit_transform(train_features)\n",
    "    validate_features_scaled = scaler.transform(validate_features)\n",
    "    \n",
    "    print(f\"Size of features before Lasso: {train_features_scaled.shape}\")\n",
    "\n",
    "    lasso = LassoCV(cv=5, random_state=42, max_iter=100, alphas=np.logspace(-4, -0.5, 30)).fit(train_features_scaled, train_labels)\n",
    "    \n",
    "    mask = lasso.coef_ != 0\n",
    "    train_features_selected = train_features_scaled[:, mask]\n",
    "    validate_features_selected = validate_features_scaled[:, mask]\n",
    "\n",
    "    if train_features_selected.shape[1] == 0:\n",
    "        print(f\"No features selected for modality {modality_used}. Skipping this combination.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Size of features after Lasso: {train_features_selected.shape}\")\n",
    "\n",
    "    # Define PyCox model\n",
    "    net = MLPVanilla(train_features_selected.shape[1], [32, 32], 1, torch.nn.ReLU)\n",
    "    model = CoxPH(net, ttoptim.Adam)\n",
    "    \n",
    "    # Prepare the dataset\n",
    "    train_data = (train_features_selected, train_labels)\n",
    "    val_data = (validate_features_selected, validate_features[:, 0])  # Assuming validate_features contains survival time and event\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(train_data, epochs=100, batch_size=256, val_data=val_data, val_batch_size=256)\n",
    "    \n",
    "    # Make predictions\n",
    "    surv = model.predict_surv_df(validate_features_selected)\n",
    "    \n",
    "    # Evaluate predictions\n",
    "    ev = EvalSurv(surv, validate_features[:, 0], validate_features[:, 1], censor_surv='km')\n",
    "    c_index = ev.concordance_td('antolini')\n",
    "    \n",
    "    # Save results\n",
    "    make_csv(surv.median().values, modality_used)\n",
    "    save_model_and_parameters(model, modality_used, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array loaded from '../radiomics features/all/radiomics_train.npy'\n",
      "Array loaded from '../radiomics features/all/radiomics_validate.npy'\n",
      "radiomic_train_features (235, 400)\n",
      "radiomic_validation_features (28, 400)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "radiomic_train_features = load_radiomic_features(\"../radiomics features/all\", \"radiomics_train.npy\")\n",
    "radiomic_validation_features = load_radiomic_features(\"../radiomics features/all\", \"radiomics_validate.npy\")\n",
    "\n",
    "print(\"radiomic_train_features\", radiomic_train_features.shape)\n",
    "print(\"radiomic_validation_features\", radiomic_validation_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and combining features... \n",
      " local-flair_t1ce_t2\n",
      " global-flair_t1ce_t2\n",
      "global_train_features (236, 744) local_train_features (236, 192)\n",
      "global_validate_features (29, 744) local_validation_features (29, 192)\n",
      "combining all\n",
      "radiomics_local_global_training (235, 1336)\n",
      "radiomics_local_global_validation (28, 1336)\n",
      "Size of features before Lasso: (235, 1336)\n",
      "Size of features after Lasso: (235, 670)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mradiomics_local_global_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m, validation_all_features\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     28\u001b[0m modality_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m modality_used_global \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m___local_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m modality_used_local\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_all_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_all_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodality_used\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 29\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_features, validate_features, train_labels, modality_used)\u001b[0m\n\u001b[1;32m     26\u001b[0m val_data \u001b[38;5;241m=\u001b[39m (validate_features_selected, validate_features[:, \u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Assuming validate_features contains survival time and event\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     32\u001b[0m surv \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_surv_df(validate_features_selected)\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'target'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for modality_used_global, modality_used_local in zip(Light_GBM_global, Light_GBM_local):\n",
    "    modality_key_local = modality_used_local.split(\"_\")\n",
    "    modality_keys_list_global_features = modality_used_global.split(\"_\")\n",
    "    print(f\"\\nLoading and combining features... \\n local-{modality_used_local}\\n global-{modality_used_global}\")\n",
    "\n",
    "    local_train_features = load_and_combine_features(modality_key_local, 'train')\n",
    "    local_validation_features = load_and_combine_features(modality_key_local, 'validation')\n",
    "    \n",
    "    global_train_features, global_validate_features, train_labels = load_features(modality_used_global)\n",
    "    \n",
    "    print(\"global_train_features\", global_train_features.shape, \"local_train_features\", local_train_features.shape)\n",
    "    print(\"global_validate_features\", global_validate_features.shape, \"local_validation_features\", local_validation_features.shape)\n",
    "\n",
    "    local_global_training_features = np.concatenate((global_train_features, local_train_features), axis=1)\n",
    "    local_global_validation_features = np.concatenate((global_validate_features, local_validation_features), axis=1)\n",
    "\n",
    "    train_labels = np.delete(train_labels, 98, axis=0)\n",
    "    local_global_training_features = np.delete(local_global_training_features, 98, axis=0)\n",
    "    local_global_validation_features = np.delete(local_global_validation_features, 27, axis=0)\n",
    "\n",
    "    training_all_features = np.concatenate((local_global_training_features, radiomic_train_features), axis=1)\n",
    "    validation_all_features = np.concatenate((local_global_validation_features, radiomic_validation_features), axis=1)\n",
    "\n",
    "    print(\"combining all\")\n",
    "    print(\"radiomics_local_global_training\", training_all_features.shape)\n",
    "    print(\"radiomics_local_global_validation\", validation_all_features.shape)\n",
    "\n",
    "    modality_used = 'global_' + modality_used_global + '___local_' + modality_used_local\n",
    "    \n",
    "    train_model(training_all_features, validation_all_features, train_labels, modality_used)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
