{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import math\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading MRI data as 2D slices.\"\"\"\n",
    "    def __init__(self, file_list, K, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.K = K\n",
    "        self.transform = transform\n",
    "        self.slices = []\n",
    "        \n",
    "        for file in tqdm(file_list, desc=\"Loading files\"):\n",
    "            img = nib.load(file).get_fdata()\n",
    "            for i in range(img.shape[2]):  # Assuming the third dimension is for slices\n",
    "                self.slices.append((file, i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file, slice_index = self.slices[index]\n",
    "        img = nib.load(file).get_fdata()[:, :, slice_index]\n",
    "        \n",
    "        img_list = []\n",
    "        if self.transform:\n",
    "            for _ in range(self.K):\n",
    "                img_transformed = self.transform(Image.fromarray(img.astype(np.uint8)))\n",
    "                img_list.append(img_transformed)\n",
    "        else:\n",
    "            img_list = [torch.from_numpy(img).float().unsqueeze(0) for _ in range(self.K)]\n",
    "        \n",
    "        return img_list, 0  # 0 is a dummy target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv4(torch.nn.Module):\n",
    "    \"\"\"A simple 4 layers CNN.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Conv4, self).__init__()\n",
    "        self.feature_size = 64\n",
    "        self.name = \"conv4\"\n",
    "\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.layer1(x)\n",
    "        h = self.layer2(h)\n",
    "        h = self.layer3(h)\n",
    "        h = self.layer4(h)\n",
    "        h = self.flatten(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalReasoning(torch.nn.Module):\n",
    "  \"\"\"Self-Supervised Relational Reasoning.\n",
    "  Essential implementation of the method, which uses\n",
    "  the 'cat' aggregation function (the most effective),\n",
    "  and can be used with any backbone.\n",
    "  \"\"\"\n",
    "  def __init__(self, backbone, feature_size=64):\n",
    "    super(RelationalReasoning, self).__init__()\n",
    "    self.backbone = backbone\n",
    "    self.relation_head = torch.nn.Sequential(\n",
    "                             torch.nn.Linear(feature_size*2, 256),\n",
    "                             torch.nn.BatchNorm1d(256),\n",
    "                             torch.nn.LeakyReLU(),\n",
    "                             torch.nn.Linear(256, 1))\n",
    "\n",
    "  def aggregate(self, features, K):\n",
    "    relation_pairs_list = list()\n",
    "    targets_list = list()\n",
    "    size = int(features.shape[0] / K)\n",
    "    shifts_counter=1\n",
    "    for index_1 in range(0, size*K, size):\n",
    "      for index_2 in range(index_1+size, size*K, size):\n",
    "        # Using the 'cat' aggregation function by default\n",
    "        pos_pair = torch.cat([features[index_1:index_1+size], \n",
    "                              features[index_2:index_2+size]], 1)\n",
    "        # Shuffle without collisions by rolling the mini-batch (negatives)\n",
    "        neg_pair = torch.cat([\n",
    "                     features[index_1:index_1+size], \n",
    "                     torch.roll(features[index_2:index_2+size], \n",
    "                     shifts=shifts_counter, dims=0)], 1)\n",
    "        relation_pairs_list.append(pos_pair)\n",
    "        relation_pairs_list.append(neg_pair)\n",
    "        targets_list.append(torch.ones(size, dtype=torch.float32))\n",
    "        targets_list.append(torch.zeros(size, dtype=torch.float32))\n",
    "        shifts_counter+=1\n",
    "        if(shifts_counter>=size): \n",
    "            shifts_counter=1 # avoid identity pairs\n",
    "    relation_pairs = torch.cat(relation_pairs_list, 0)\n",
    "    targets = torch.cat(targets_list, 0)\n",
    "    return relation_pairs, targets\n",
    "\n",
    "  def train(self, tot_epochs, train_loader):\n",
    "    optimizer = torch.optim.Adam([\n",
    "                  {'params': self.backbone.parameters()},\n",
    "                  {'params': self.relation_head.parameters()}])                               \n",
    "    BCE = torch.nn.BCEWithLogitsLoss()\n",
    "    self.backbone.train()\n",
    "    self.relation_head.train()\n",
    "    for epoch in range(tot_epochs):\n",
    "      # the real target is discarded (unsupervised)\n",
    "      for i, (data_augmented, _) in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{tot_epochs}\"):\n",
    "        K = len(data_augmented) # tot augmentations\n",
    "        x = torch.cat(data_augmented, 0)\n",
    "        optimizer.zero_grad()              \n",
    "        # forward pass (backbone)\n",
    "        features = self.backbone(x) \n",
    "        # aggregation function\n",
    "        relation_pairs, targets = self.aggregate(features, K)\n",
    "        # forward pass (relation head)\n",
    "        score = self.relation_head(relation_pairs).squeeze()        \n",
    "        # cross-entropy loss and backward\n",
    "        loss = BCE(score, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()            \n",
    "        # estimate the accuracy\n",
    "        predicted = torch.round(torch.sigmoid(score))\n",
    "        correct = predicted.eq(targets.view_as(predicted)).sum()\n",
    "        accuracy = (100.0 * correct / float(len(targets)))\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "          print(f'Batch [{i+1}/{len(train_loader)}] - Loss: {loss.item():.5f}; Accuracy: {accuracy.item():.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../dataset/MICCAI_BraTS2020_TrainingData/'\n",
    "modality_keys = [\"flair\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a list of data dictionaries\n",
    "def create_data_list(data_dir):\n",
    "    data_list = []\n",
    "    patients = os.listdir(data_dir)\n",
    "    for patient in tqdm(patients, desc=\"Creating data list\"):\n",
    "        patient_dir = os.path.join(data_dir, patient)\n",
    "        if os.path.isdir(patient_dir):\n",
    "            data_dict = os.path.join(patient_dir, f\"{patient}_flair.nii\")\n",
    "            data_list.append(data_dict)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "K = 4\n",
    "batch_size = 64\n",
    "tot_epochs = 1\n",
    "feature_size = 64\n",
    "\n",
    "# Transformations for MRI slices\n",
    "normalize = transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating data list: 100%|██████████| 371/371 [00:00<00:00, 2403.43it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "backbone = Conv4()\n",
    "model = RelationalReasoning(backbone, feature_size)\n",
    "\n",
    "# Replace this with your actual list of .nii files\n",
    "file_list = create_data_list(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 369/369 [04:54<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_set = MRIDataset(file_list=file_list, K=K, transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 1/894 [00:08<2:01:20,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [1/894] - Loss: 0.68923; Accuracy: 54.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  11%|█▏        | 101/894 [13:23<1:32:39,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [101/894] - Loss: 0.14401; Accuracy: 96.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  22%|██▏       | 201/894 [24:49<1:20:15,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [201/894] - Loss: 0.10423; Accuracy: 97.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  34%|███▎      | 301/894 [36:00<1:04:40,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [301/894] - Loss: 0.08755; Accuracy: 97.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  45%|████▍     | 401/894 [47:22<58:20,  7.10s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [401/894] - Loss: 0.07395; Accuracy: 97.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  56%|█████▌    | 501/894 [57:57<30:25,  4.64s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [501/894] - Loss: 0.05847; Accuracy: 98.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  67%|██████▋   | 601/894 [1:06:05<23:21,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [601/894] - Loss: 0.10885; Accuracy: 96.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  78%|███████▊  | 701/894 [1:14:09<15:45,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [701/894] - Loss: 0.04078; Accuracy: 98.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  90%|████████▉ | 801/894 [1:22:18<07:19,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [801/894] - Loss: 0.04672; Accuracy: 98.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 894/894 [1:29:54<00:00,  6.03s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.train(tot_epochs=tot_epochs, train_loader=train_loader)\n",
    "torch.save(model.backbone.state_dict(), './backbone_mri.tar')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
