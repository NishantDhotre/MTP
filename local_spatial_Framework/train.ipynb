{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = (1 - pt) ** self.gamma * BCE_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return F_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return F_loss.sum()\n",
    "        else:\n",
    "            return F_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIPatchDataset(Dataset):\n",
    "    def __init__(self, image_dir, patch_size=32, transform=None, K=2):\n",
    "        self.image_dir = image_dir\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform\n",
    "        self.K = K\n",
    "        self.image_files = [os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_files[idx]).convert('RGB')\n",
    "        w, h = img.size\n",
    "        patches = []\n",
    "        for _ in range(self.K):\n",
    "            left = np.random.randint(0, w - self.patch_size)\n",
    "            top = np.random.randint(0, h - self.patch_size)\n",
    "            patch = img.crop((left, top, left + self.patch_size, top + self.patch_size))\n",
    "            if self.transform:\n",
    "                patch = self.transform(patch)\n",
    "            patches.append(patch)\n",
    "        return patches, idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(dataset):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    for i in range(len(dataset)):\n",
    "        patches, idx = dataset[i]\n",
    "        for k in range(len(patches)):\n",
    "            for j in range(k + 1, len(patches)):\n",
    "                # Positive pair\n",
    "                pairs.append((patches[k], patches[j]))\n",
    "                labels.append(1.0)\n",
    "                # Negative pair\n",
    "                neg_idx = np.random.randint(0, len(dataset))\n",
    "                while neg_idx == idx:\n",
    "                    neg_idx = np.random.randint(0, len(dataset))\n",
    "                neg_patches, _ = dataset[neg_idx]\n",
    "                neg_patch = neg_patches[np.random.randint(0, len(neg_patches))]\n",
    "                pairs.append((patches[k], neg_patch))\n",
    "                labels.append(0.0)\n",
    "    return pairs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv4, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.layer1(x)\n",
    "        h = self.layer2(h)\n",
    "        h = self.layer3(h)\n",
    "        h = self.layer4(h)\n",
    "        h = torch.flatten(h, 1)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalReasoning(torch.nn.Module):\n",
    "    def __init__(self, backbone, feature_size=64):\n",
    "        super(RelationalReasoning, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.relation_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(feature_size * 2, 256),\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        features1 = self.backbone(x1)\n",
    "        features2 = self.backbone(x2)\n",
    "        combined_features = torch.cat([features1, features2], dim=1)\n",
    "        score = self.relation_head(combined_features)\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, pairs, labels, epochs=10, batch_size=64, learning_rate=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = FocalLoss()\n",
    "    dataset = list(zip(pairs, labels))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (pair, label) in enumerate(dataloader):\n",
    "            img1, img2 = zip(*pair)\n",
    "            img1 = torch.stack(img1)\n",
    "            img2 = torch.stack(img2)\n",
    "            label = torch.tensor(label).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(img1, img2).squeeze()\n",
    "            loss = criterion(outputs, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(dataloader):.4f}, Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script\n",
    "image_dir = 'path_to_mri_images'  # Update with the actual path\n",
    "dataset = MRIPatchDataset(image_dir, transform=transform, K=2)\n",
    "pairs, labels = create_pairs(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
