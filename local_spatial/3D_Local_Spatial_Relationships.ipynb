{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import math\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading MRI data as 3D volumes.\"\"\"\n",
    "    def __init__(self, file_list, K, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.K = K\n",
    "        self.transform = transform\n",
    "        self.volumes = []\n",
    "        \n",
    "        for file in tqdm(file_list, desc=\"Loading files\"):\n",
    "            img = nib.load(file).get_fdata()\n",
    "            self.volumes.append(img)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.volumes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.volumes[index]\n",
    "        \n",
    "        img_list = []\n",
    "        if self.transform:\n",
    "            for _ in range(self.K):\n",
    "                img_transformed = self.transform(Image.fromarray(img.astype(np.uint8)))\n",
    "                img_list.append(img_transformed)\n",
    "        else:\n",
    "            img_list = [torch.from_numpy(img).float().unsqueeze(0) for _ in range(self.K)]\n",
    "        \n",
    "        return img_list, 0  # 0 is a dummy target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv4_3D(torch.nn.Module):\n",
    "    \"\"\"A simple 4 layers 3D CNN.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Conv4_3D, self).__init__()\n",
    "        self.feature_size = 64\n",
    "        self.name = \"conv4_3d\"\n",
    "\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(1, 8, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm3d(8),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool3d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(8, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm3d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool3d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm3d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool3d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm3d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AdaptiveAvgPool3d(1)\n",
    "        )\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv3d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, torch.nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.layer1(x)\n",
    "        h = self.layer2(h)\n",
    "        h = self.layer3(h)\n",
    "        h = self.layer4(h)\n",
    "        h = self.flatten(h)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalReasoning(torch.nn.Module):\n",
    "    \"\"\"Self-Supervised Relational Reasoning for 3D MRI data.\"\"\"\n",
    "    def __init__(self, backbone, feature_size=64):\n",
    "        super(RelationalReasoning, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.relation_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(feature_size * 2, 256),\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def aggregate(self, features, K):\n",
    "        relation_pairs_list = list()\n",
    "        targets_list = list()\n",
    "        size = int(features.shape[0] / K)\n",
    "        shifts_counter = 1\n",
    "        for index_1 in range(0, size * K, size):\n",
    "            for index_2 in range(index_1 + size, size * K, size):\n",
    "                # Using the 'cat' aggregation function by default\n",
    "                pos_pair = torch.cat([features[index_1:index_1 + size], features[index_2:index_2 + size]], 1)\n",
    "                # Shuffle without collisions by rolling the mini-batch (negatives)\n",
    "                neg_pair = torch.cat([features[index_1:index_1 + size], torch.roll(features[index_2:index_2 + size], shifts=shifts_counter, dims=0)], 1)\n",
    "                relation_pairs_list.append(pos_pair)\n",
    "                relation_pairs_list.append(neg_pair)\n",
    "                targets_list.append(torch.ones(size, dtype=torch.float32))\n",
    "                targets_list.append(torch.zeros(size, dtype=torch.float32))\n",
    "                shifts_counter += 1\n",
    "                if shifts_counter >= size:\n",
    "                    shifts_counter = 1  # avoid identity pairs\n",
    "        relation_pairs = torch.cat(relation_pairs_list, 0)\n",
    "        targets = torch.cat(targets_list, 0)\n",
    "        return relation_pairs, targets\n",
    "\n",
    "    def train_model(self, tot_epochs, train_loader):\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': self.backbone.parameters()},\n",
    "            {'params': self.relation_head.parameters()}\n",
    "        ])\n",
    "        BCE = torch.nn.BCEWithLogitsLoss()\n",
    "        self.backbone.train()\n",
    "        self.relation_head.train()\n",
    "        for epoch in range(tot_epochs):\n",
    "            for i, (data_augmented, _) in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}/{tot_epochs}\"):\n",
    "                K = len(data_augmented)  # total augmentations\n",
    "                x = torch.cat(data_augmented, 0)\n",
    "                optimizer.zero_grad()\n",
    "                # forward pass (backbone)\n",
    "                features = self.backbone(x)\n",
    "                # aggregation function\n",
    "                relation_pairs, targets = self.aggregate(features, K)\n",
    "                # forward pass (relation head)\n",
    "                score = self.relation_head(relation_pairs).squeeze()\n",
    "                # cross-entropy loss and backward\n",
    "                loss = BCE(score, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # estimate the accuracy\n",
    "                predicted = torch.round(torch.sigmoid(score))\n",
    "                correct = predicted.eq(targets.view_as(predicted)).sum()\n",
    "                accuracy = (100.0 * correct / float(len(targets)))\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(f'Batch [{i + 1}/{len(train_loader)}] - Loss: {loss.item():.5f}; Accuracy: {accuracy.item():.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_list(data_dir):\n",
    "    data_list = []\n",
    "    patients = os.listdir(data_dir)\n",
    "    for patient in tqdm(patients, desc=\"Creating data list\"):\n",
    "        patient_dir = os.path.join(data_dir, patient)\n",
    "        if os.path.isdir(patient_dir):\n",
    "            data_dict = os.path.join(patient_dir, f\"{patient}_flair.nii\")\n",
    "            data_list.append(data_dict)\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating data list: 100%|██████████| 371/371 [00:00<00:00, 1209.90it/s]\n",
      "Loading files: 100%|██████████| 369/369 [00:19<00:00, 18.84it/s]\n",
      "Epoch 1/10:   1%|          | 1/93 [00:25<38:20, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [1/93] - Loss: 0.72551; Accuracy: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  52%|█████▏    | 48/93 [19:31<17:43, 23.62s/it]"
     ]
    }
   ],
   "source": [
    "# Paths and Hyper-parameters\n",
    "train_path = '../dataset/MICCAI_BraTS2020_TrainingData/'\n",
    "K = 4\n",
    "batch_size = 4  # Reduced batch size due to 3D data's high memory consumption\n",
    "tot_epochs = 10\n",
    "feature_size = 64\n",
    "\n",
    "# Transformations for MRI volumes (adapted for 3D)\n",
    "train_transform = None  # Define your transformations if needed\n",
    "\n",
    "# Initialize the backbone and the relational reasoning model\n",
    "backbone = Conv4_3D()\n",
    "model = RelationalReasoning(backbone, feature_size)\n",
    "\n",
    "# Create the list of files and the dataset\n",
    "file_list = create_data_list(train_path)\n",
    "train_set = MRIDataset(file_list=file_list, K=K, transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "model.train_model(tot_epochs=tot_epochs, train_loader=train_loader)\n",
    "torch.save(model.backbone.state_dict(), './backbone_mri_3d.tar')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
