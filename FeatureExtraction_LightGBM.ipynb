{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import monai\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, Spacingd, Orientationd, ScaleIntensityRanged, CropForegroundd,\n",
    "    RandCropByPosNegLabeld, RandFlipd, RandRotate90d, RandShiftIntensityd, EnsureTyped, DivisiblePadd,ResizeWithPadOrCropd\n",
    ")\n",
    "from monai.data import DataLoader, CacheDataset\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.utils import set_determinism\n",
    "from monai.data.image_reader import NibabelReader\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import csv\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define directories\n",
    "train_path = 'dataset/MICCAI_BraTS2020_TrainingData/'\n",
    "val_path = 'dataset/MICCAI_BraTS2020_ValidationData/'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_list(data_dir, patient_ids, lables, modality_keys):\n",
    "    data_list = []\n",
    "    for idx, patient in enumerate(patient_ids):\n",
    "        patient_dir = f\"{data_dir}{patient}/\"\n",
    "        if os.path.isdir(patient_dir):\n",
    "            data_dict = {key: os.path.join(patient_dir, f\"{patient}_{key}.nii\") for key in modality_keys}\n",
    "            data_dict['lable'] = lables[idx]\n",
    "            data_list.append(data_dict)\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_list_val(data_dir,  modality_keys):\n",
    "    df = pd.read_csv(f'{val_path}/survival_evaluation.csv')\n",
    "    patient_ids = df['BraTS20ID'].values\n",
    "    data_list = []\n",
    "    for idx, patient in enumerate(patient_ids):\n",
    "        patient_dir = f\"{data_dir}{patient}/\"\n",
    "        if os.path.isdir(patient_dir):\n",
    "            data_dict = {key: os.path.join(patient_dir, f\"{patient}_{key}.nii\") for key in modality_keys}\n",
    "            # data_dict['lable'] = lables[idx]\n",
    "            data_list.append(data_dict)\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    def extract_number(value):\n",
    "        if isinstance(value, str):\n",
    "            match = re.search(r'\\d+', value)\n",
    "            return int(match.group()) if match else None\n",
    "        return value\n",
    "\n",
    "    df['Survival_days'] = df['Survival_days'].apply(extract_number)\n",
    "    df['Survival_days'] = pd.to_numeric(df['Survival_days'], errors='coerce')\n",
    "    df = df.dropna(subset=['Survival_days'])\n",
    "    df['Survival_days'] = df['Survival_days'].astype(int)\n",
    "    \n",
    "    return df['Survival_days'].values, df['Brats20ID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# since we dont have lables for validation dataset not using it for validation\n",
    "# valdate_data_list = create_data_list_val(val_path, modality_keys)\n",
    "# val_labels = preprocess_labels(f'{val_path}/survival_evaluation.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nibabel as nib\n",
    "# import torch\n",
    "\n",
    "# # Load the NIfTI image\n",
    "# file_path = train_data_list[0]['flair']\n",
    "# img = nib.load(file_path)\n",
    "\n",
    "# # Get the image data as a numpy array   \n",
    "# img_data = img.get_fdata()\n",
    "\n",
    "# # Convert the numpy array to a PyTorch tensor\n",
    "# img_tensor = torch.tensor(img_data, dtype=torch.float32)\n",
    "\n",
    "# # Print the shape of the tensor\n",
    "# print(f'Tensor shape: {img_tensor.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(modality_keys, pixdim=(1.0, 1.0, 1.0), is_train=True):\n",
    "    transform_list = [\n",
    "        LoadImaged(keys=modality_keys, reader=NibabelReader()),\n",
    "        EnsureChannelFirstd(keys=modality_keys),\n",
    "        Spacingd(keys=modality_keys, pixdim=pixdim, mode=(\"bilinear\")),\n",
    "        Orientationd(keys=modality_keys, axcodes=\"RAS\"),\n",
    "        ScaleIntensityRanged(keys=modality_keys, a_min=-175, a_max=250, b_min=0.0, b_max=1.0, clip=True),\n",
    "        CropForegroundd(keys=modality_keys, source_key=modality_keys[0], allow_smaller=True),\n",
    "        ResizeWithPadOrCropd(keys=modality_keys, spatial_size=(256, 256, 160)),  # Kept original size\n",
    "        EnsureTyped(keys=modality_keys),\n",
    "    ]\n",
    "    \n",
    "    if is_train:\n",
    "        transform_list.extend([\n",
    "            RandFlipd(keys=modality_keys, prob=0.5, spatial_axis=0),\n",
    "            RandFlipd(keys=modality_keys, prob=0.5, spatial_axis=1),\n",
    "            RandFlipd(keys=modality_keys, prob=0.5, spatial_axis=2),\n",
    "            RandRotate90d(keys=modality_keys, prob=0.5, max_k=3),\n",
    "            RandShiftIntensityd(keys=modality_keys, offsets=0.10, prob=0.5),\n",
    "        ])\n",
    "    \n",
    "    return Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractorSwinUNETR(SwinUNETR):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def extract_features(self, x):\n",
    "        hidden_states = self.swinViT(x, self.normalize)\n",
    "        # print(\"starting ------\")\n",
    "        # Adaptive pooling to resize all hidden states to a common size\n",
    "        i = 0\n",
    "        pooled_states = []\n",
    "        for state in hidden_states:\n",
    "            # Adaptive average pooling to 4x4x4\n",
    "            # print('At',i,\"state\", state.shape)\n",
    "            i = i + 1\n",
    "            pooled = F.adaptive_avg_pool3d(state, (4, 4, 4))\n",
    "            pooled_states.append(pooled)\n",
    "        # print(\"end ====\\n\")\n",
    "        # Concatenate the pooled states\n",
    "        return torch.cat(pooled_states, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(y_pred_validation, modality_used):\n",
    "    df = pd.read_csv(f'{val_path}/survival_evaluation.csv')\n",
    "    validation_ids = df['BraTS20ID'].values\n",
    "    \n",
    "    filename = f\"./global_predictons/Light_GBM/{modality_used}Light_GBM.csv\"\n",
    "\n",
    "    # Writing to csv file\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"ID\", \"Days\"])  # Writing the header\n",
    "        for id, day in zip(validation_ids, y_pred_validation):\n",
    "            writer.writerow([id, day])\n",
    "\n",
    "    print(f\"CSV file '{filename}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_dataset(modality_keys, dataloader, feature_extractor):\n",
    "    features = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        inputs = torch.cat([batch[key] for key in modality_keys], dim=1).to(device)\n",
    "        with torch.no_grad():\n",
    "            feature = feature_extractor.extract_features(inputs)\n",
    "        \n",
    "        # Global average pooling to reduce spatial dimensions\n",
    "        feature = torch.mean(feature, dim=[2, 3, 4])\n",
    "        \n",
    "        features.append(feature.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(modality_keys, train_data_list, valdate_data_list):\n",
    "\n",
    "    train_transforms = get_transforms(modality_keys, is_train=True)\n",
    "    val_transforms = get_transforms(modality_keys, is_train=False)\n",
    "        \n",
    "    train_ds = CacheDataset(\n",
    "        data=train_data_list,\n",
    "        transform=train_transforms,\n",
    "        cache_rate=0.5,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    train_loader = DataLoader(train_ds,  batch_size=2, shuffle=True, num_workers=4)\n",
    "\n",
    "    validate_ds = CacheDataset(\n",
    "        data=valdate_data_list,\n",
    "        transform=val_transforms,\n",
    "        cache_rate=0.5,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    validate_loader = DataLoader(validate_ds, batch_size=2, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Load the pretrained model\n",
    "    \n",
    "    model = SwinUNETR(\n",
    "        img_size=(256, 256, 160),  # Kept original size\n",
    "        in_channels=len(modality_keys),\n",
    "        out_channels=len(modality_keys),\n",
    "        feature_size=24,  # Reduced from 48 to save memory\n",
    "        use_checkpoint=True,\n",
    "    ).to(device)\n",
    "\n",
    "    # Load the saved weights\n",
    "    modality_used = \"_\".join(modality_keys)\n",
    "    model_save_path = f\"model_saved/swin_unetr_{modality_used}_best.pth\"\n",
    "\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    model.eval()\n",
    "\n",
    "    #  Create an instance of the feature extractor\n",
    "    feature_extractor = FeatureExtractorSwinUNETR(\n",
    "        img_size=(256, 256, 160),  # Kept original size\n",
    "        in_channels=len(modality_keys),\n",
    "        out_channels=len(modality_keys),\n",
    "        feature_size=24,  # Reduced from 48 to save memory\n",
    "        use_checkpoint=True,\n",
    "    ).to(device)\n",
    "    feature_extractor.load_state_dict(model.state_dict())\n",
    "    feature_extractor.eval()\n",
    "\n",
    "\n",
    "    # Extract features for training and validation sets\n",
    "    train_features = extract_features_from_dataset(modality_keys, train_loader, feature_extractor)\n",
    "\n",
    "    validate_features = extract_features_from_dataset(validate_loader, feature_extractor)\n",
    "\n",
    "\n",
    "    # Preprocess the data\n",
    "    scaler = StandardScaler()\n",
    "    train_features_scaled = scaler.fit_transform(train_features)\n",
    "    validate_features_scaled = scaler.transform(validate_features)\n",
    "\n",
    "    # Hyperparameter tuning for LightGBM\n",
    "    param_dist = {\n",
    "        'num_leaves': [31, 63, 127],\n",
    "        'max_depth': [-1, 5, 10, 20],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'min_child_samples': [10, 20, 30]\n",
    "    }\n",
    "\n",
    "    # Train LightGBM regression model\n",
    "    lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "    random_search = RandomizedSearchCV(lgb_model, param_distributions=param_dist, n_iter=20, cv=5, random_state=42, n_jobs=-1)\n",
    "    random_search.fit(train_features_scaled, train_labels)\n",
    "\n",
    "    # making prediction for validation data\n",
    "    y_pred_validation = random_search.predict(validate_features_scaled)\n",
    "\n",
    "    make_csv(y_pred_validation, modality_used)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_ds = CacheDataset(\n",
    "#     data=val_data_list,\n",
    "#     transform=val_transforms,\n",
    "#     cache_rate=0.5,\n",
    "#     num_workers=4,\n",
    "# )\n",
    "# val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def evaluate_predictions(y_true, y_pred):\n",
    "#     # Calculate metrics\n",
    "#     accuracy = accuracy_score(y_true, y_pred)\n",
    "#     precision = precision_score(y_true, y_pred, average='weighted')\n",
    "#     recall = recall_score(y_true, y_pred, average='weighted')\n",
    "#     f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "#     conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "#     class_report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    \n",
    "#     # Print metrics\n",
    "#     print(f'Accuracy: {accuracy:.2f}')\n",
    "#     print(f'Precision: {precision:.2f}')\n",
    "#     print(f'Recall: {recall:.2f}')\n",
    "#     print(f'F1 Score: {f1:.2f}')\n",
    "#     print('Confusion Matrix:')\n",
    "#     print(conf_matrix)\n",
    "#     print('Classification Report:')\n",
    "#     print(classification_report(y_true, y_pred))\n",
    "    \n",
    "#     # Return metrics in a dictionary\n",
    "#     return {\n",
    "#         'accuracy': accuracy,\n",
    "#         'precision': precision,\n",
    "#         'recall': recall,\n",
    "#         'f1_score': f1,\n",
    "#         'confusion_matrix': conf_matrix,\n",
    "#         'classification_report': class_report\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def categorize_days(days):\n",
    "#     categorized_days = []\n",
    "#     for day in days:\n",
    "#         if day > 455:\n",
    "#             categorized_days.append(2)\n",
    "#         elif day < 304:\n",
    "#             categorized_days.append(0)\n",
    "#         else:\n",
    "#             categorized_days.append(1)\n",
    "#     return categorized_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "# y_pred_1 = random_search.predict(val_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_validation = random_search.predict(validate_features_scaled)\n",
    "# validation_res = categorize_days(y_pred_validation)\n",
    "# real_lable = categorize_days(val_labels)\n",
    "# results_1 = evaluate_predictions(real_lable, pred_lable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_lable = categorize_days(y_pred_1)\n",
    "# real_lable = categorize_days(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_1 = evaluate_predictions(real_lable, pred_lable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model\n",
    "# mae = mean_absolute_error(val_labels, y_pred_1)\n",
    "# rmse = mean_squared_error(val_labels, y_pred_1, squared=False)\n",
    "# print(f\"Mean Absolute Error: {mae}\")\n",
    "# print(f\"Root Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_keys_list = [\n",
    "        [\"flair\"],\n",
    "        [\"t1ce\"],\n",
    "        [\"flair\", \"t1ce\"],\n",
    "        [\"flair\", \"t1ce\", \"t2\"],\n",
    "        [\"flair\", \"t1\", \"t1ce\", \"t2\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_id = preprocess_labels(f'{train_path}/survival_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 118/118 [02:12<00:00,  1.12s/it]\n",
      "Loading dataset: 100%|██████████| 14/14 [00:22<00:00,  1.62s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m train_data_list \u001b[38;5;241m=\u001b[39m create_data_list(train_path, train_id, train_labels, modality_keys)\n\u001b[1;32m      5\u001b[0m valdate_data_list \u001b[38;5;241m=\u001b[39m create_data_list_val(val_path, modality_keys)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodality_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvaldate_data_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 52\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(modality_keys, train_data_list, valdate_data_list)\u001b[0m\n\u001b[1;32m     48\u001b[0m feature_extractor\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Extract features for training and validation sets\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m train_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_from_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodality_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m validate_features \u001b[38;5;241m=\u001b[39m extract_features_from_dataset(validate_loader, feature_extractor)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Preprocess the data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m, in \u001b[0;36mextract_features_from_dataset\u001b[0;34m(modality_keys, dataloader, feature_extractor)\u001b[0m\n\u001b[1;32m      5\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([batch[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m modality_keys], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     feature \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Global average pooling to reduce spatial dimensions\u001b[39;00m\n\u001b[1;32m     10\u001b[0m feature \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(feature, dim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m])\n",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m, in \u001b[0;36mFeatureExtractorSwinUNETR.extract_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 6\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswinViT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# print(\"starting ------\")\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Adaptive pooling to resize all hidden states to a common size\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/monai/networks/nets/swin_unetr.py:1067\u001b[0m, in \u001b[0;36mSwinTransformer.forward\u001b[0;34m(self, x, normalize)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_v2:\n\u001b[1;32m   1066\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers1c[\u001b[38;5;241m0\u001b[39m](x0\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[0;32m-> 1067\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m x1_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(x1, normalize)\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_v2:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/monai/networks/nets/swin_unetr.py:903\u001b[0m, in \u001b[0;36mBasicLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    901\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m compute_mask([dp, hp, wp], window_size, shift_size, x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 903\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    904\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(b, d, h, w, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/monai/networks/nets/swin_unetr.py:696\u001b[0m, in \u001b[0;36mSwinTransformerBlock.forward\u001b[0;34m(self, x, mask_matrix)\u001b[0m\n\u001b[1;32m    694\u001b[0m shortcut \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_checkpoint:\n\u001b[0;32m--> 696\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_part1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_reentrant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    698\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_part1(x, mask_matrix)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/_dynamo/external_utils.py:36\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:494\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# Runs pre-forward logic\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28mnext\u001b[39m(gen)\n\u001b[0;32m--> 494\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# Runs post-forward logic\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/monai/networks/nets/swin_unetr.py:636\u001b[0m, in \u001b[0;36mSwinTransformerBlock.forward_part1\u001b[0;34m(self, x, mask_matrix)\u001b[0m\n\u001b[1;32m    634\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    635\u001b[0m x_windows \u001b[38;5;241m=\u001b[39m window_partition(shifted_x, window_size)\n\u001b[0;32m--> 636\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m attn_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m(window_size \u001b[38;5;241m+\u001b[39m (c,)))\n\u001b[1;32m    638\u001b[0m shifted_x \u001b[38;5;241m=\u001b[39m window_reverse(attn_windows, window_size, dims)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/monai/networks/nets/swin_unetr.py:529\u001b[0m, in \u001b[0;36mWindowAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    527\u001b[0m     nw \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    528\u001b[0m     attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mview(b \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m nw, nw, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, n, n) \u001b[38;5;241m+\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 529\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, n, n)\n\u001b[1;32m    530\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(attn)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for modality_keys in modality_keys_list:\n",
    "    print(\"now working on\", modality_keys)\n",
    "    in_channels = len(modality_keys)\n",
    "    out_channels = len(modality_keys)\n",
    "    train_data_list = create_data_list(train_path, train_id, train_labels, modality_keys)\n",
    "    valdate_data_list = create_data_list_val(val_path, modality_keys)\n",
    "     \n",
    "    build_model(modality_keys, train_data_list, valdate_data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modality_keys = [\"flair\", \"t1ce\", \"t2\"]\n",
    "# in_channels = len(modality_keys)\n",
    "# out_channels = len(modality_keys) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels, train_id = preprocess_labels(f'{train_path}/survival_info.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_data_list = create_data_list(train_path, train_id, train_labels, modality_keys)\n",
    "# valdate_data_list = create_data_list_val(val_path, modality_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
